{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cite \n",
    "# https://stackoverflow.com/questions/17390326/getting-rid-of-stop-words-and-document-tokenization-using-nltk\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "def tokenize(text: str) -> list:\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return [t for t in tokens if t not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('/data/khodadaa/stack_results/stack_feat/18_ml_in.csv', \n",
    "                      usecols=['Query', 'Y', '18', '100', 'TestViewCount', 'ql_t', 'ql_t.1'])\n",
    "X = data_df['Query']\n",
    "y = data_df['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 59s, sys: 994 ms, total: 3min\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%time data_df['tokens'] = data_df.Query.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = model_selection.train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 362 ms, total: 12 s\n",
      "Wall time: 12 s\n",
      "CPU times: user 8.29 s, sys: 102 ms, total: 8.39 s\n",
      "Wall time: 8.39 s\n",
      "CPU times: user 3.99 s, sys: 30 ms, total: 4.02 s\n",
      "Wall time: 4.02 s\n"
     ]
    }
   ],
   "source": [
    "count_vec = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "%time count_vec.fit(data_df['Query'])\n",
    "\n",
    "%time xtrain_count = count_vec.transform(train_X)\n",
    "%time xtest_count = count_vec.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vec.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 399 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n",
      "CPU times: user 8 s, sys: 129 ms, total: 8.12 s\n",
      "Wall time: 8.12 s\n",
      "CPU times: user 3.86 s, sys: 23 ms, total: 3.89 s\n",
      "Wall time: 3.89 s\n",
      "CPU times: user 1min 18s, sys: 2.43 s, total: 1min 21s\n",
      "Wall time: 1min 15s\n",
      "CPU times: user 16.3 s, sys: 8.97 ms, total: 16.4 s\n",
      "Wall time: 16.4 s\n",
      "CPU times: user 8.02 s, sys: 3.96 ms, total: 8.03 s\n",
      "Wall time: 8.03 s\n",
      "CPU times: user 1min 17s, sys: 3.48 s, total: 1min 21s\n",
      "Wall time: 1min 21s\n",
      "CPU times: user 57.5 s, sys: 1.64 s, total: 59.1 s\n",
      "Wall time: 59.1 s\n",
      "CPU times: user 28.3 s, sys: 729 ms, total: 29 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "%time tfidf_vect.fit(data_df['Query'])\n",
    "%time xtrain_tfidf =  tfidf_vect.transform(train_X)\n",
    "%time xtest_tfidf =  tfidf_vect.transform(test_X)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "%time tfidf_vect_ngram.fit(data_df['Query'])\n",
    "%time xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_X)\n",
    "%time xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_X)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "%time tfidf_vect_ngram_chars.fit(data_df['Query'])\n",
    "%time xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_X) \n",
    "%time xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_test, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_test)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, test_y), metrics.precision_score(predictions, test_y), np.sum(predictions)/predictions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB features as a baseline\n",
    "cols = ['covered_t_18', 'mean_df_t_18', 'min_df_t_18', 'mean_mean_pop_t_18', 'mean_min_pop_t_18', 'min_mean_pop_t_18', 'min_min_pop_t_18', 'ql_t_18', 'qll_t_18', 'covered_t_bi_18', 'mean_df_t_bi_18', 'min_df_t_bi_18', 'mean_mean_pop_t_bi_18', 'mean_min_pop_t_bi_18', 'min_mean_pop_t_bi_18', 'min_min_pop_t_bi_18', 'ql_t_bi_18', 'qll_t_bi_18', 'covered_t_c18', 'mean_df_t_c18', 'min_df_t_c18', 'mean_mean_pop_t_c18', 'mean_min_pop_t_c18', 'min_mean_pop_t_c18', 'min_min_pop_t_c18', 'ql_t_c18', 'qll_t_c18', 'covered_t_bi_c18', 'mean_df_t_bi_c18', 'min_df_t_bi_c18', 'mean_mean_pop_t_bi_c18', 'mean_min_pop_t_bi_c18', 'min_mean_pop_t_bi_c18', 'min_min_pop_t_bi_c18', 'ql_t_bi_c18', 'qll_t_bi_c18', 'ql_t', 'ql_t.1']\n",
    "db_df = pd.read_csv(\"/data/khodadaa/stack_results/stack_feat/18_ml_in_ghadakcv.csv\", usecols=cols)\n",
    "xtrain_db, xtest_db = db_df.loc[train_X.index], db_df.loc[test_X.index]\n",
    "sc = preprocessing.MinMaxScaler().fit(db_df)\n",
    "xtrain_db = sc.transform(xtrain_db)\n",
    "xtest_db = sc.transform(xtest_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, DB features:  (0.5902645978312745, 0.6814455779977756, 0.48306862451303567)\n",
      "LR, Count Vectors:  (0.6163470182798921, 0.5810872042730231, 0.416425265541238)\n",
      "LR, WordLevel TF-IDF:  (0.6056254786401625, 0.5873347155666012, 0.42967180545844014)\n",
      "LR, N-Gram Vectors:  (0.6215995738021509, 0.4594735613560532, 0.3620212211012331)\n",
      "LR, CharLevel Vectors:  (0.6061277039700773, 0.5824053605019979, 0.42717732716234363)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on DB extracted features\n",
    "scores = train_model(linear_model.LogisticRegression(class_weight='balanced'), xtrain_db, train_y, xtest_db)\n",
    "print (\"LR, DB features: \", scores)\n",
    "\n",
    "# Linear Classifier on Count Vectors\n",
    "scores = train_model(linear_model.LogisticRegression(class_weight='balanced'), xtrain_count, train_y, xtest_count)\n",
    "print (\"LR, Count Vectors: \", scores)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "scores = train_model(linear_model.LogisticRegression(class_weight='balanced'), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", scores)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "scores = train_model(linear_model.LogisticRegression(class_weight='balanced'), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", scores)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "scores = train_model(linear_model.LogisticRegression(class_weight='balanced'), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", scores)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time scores = train_model(ensemble.RandomForestClassifier(class_weight= 'balanced'), xtrain_db, train_y, xtest_db)\n",
    "print (\"RF, DB features: \", scores)\n",
    "\n",
    "%time scores = train_model(ensemble.RandomForestClassifier(class_weight= 'balanced'), xtrain_count, train_y, xtest_count)\n",
    "print (\"RF, Count Vectors: \", scores)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "% time scores = train_model(ensemble.RandomForestClassifier(class_weight= 'balanced'), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", scores)\n",
    "\n",
    "CPU times: user 3min 41s, sys: 5.24 s, total: 3min 46s\n",
    "Wall time: 1min 25s\n",
    "RF, DB features:  (0.7922008013407474, 0.04174612102671038, 0.022428106860231525)\n",
    "CPU times: user 1h 2min 23s, sys: 15.1 s, total: 1h 2min 39s\n",
    "Wall time: 1h 2s\n",
    "RF, Count Vectors:  (0.7901586033141322, 0.045038618248665854, 0.025802173165073973)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/stak/users/khodadaa/miniconda3/envs/db/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 1.87 s, total: 1min 9s\n",
      "Wall time: 58.8 s\n",
      "SVM, N-Gram Vectors:  (0.20225807167671117, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/stak/users/khodadaa/miniconda3/envs/db/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 36s, sys: 4.32 s, total: 3min 40s\n",
      "Wall time: 3min 24s\n",
      "SVM, DB features:  (0.20225807167671117, 1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "%time scores = train_model(svm.SVC(cache_size=20000, max_iter=1000, class_weight='balanced'), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "print (\"SVM, N-Gram Vectors: \", scores)\n",
    "\n",
    "%time scores = train_model(svm.SVC(cache_size=20000, max_iter=1000, class_weight='balanced'), xtrain_db, train_y, xtest_db)\n",
    "print (\"SVM, DB features: \", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5422757189313977"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ql_y = data_df.loc[test_X.index, ['ql_t', 'ql_t.1']]\n",
    "ql_y['pred'] = 0\n",
    "ql_y[ql_y['ql_t'] >= ql_y['ql_t.1']] = 1\n",
    "metrics.accuracy_score(ql_y['pred'].as_matrix(), test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7979195107603858"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- np.sum(test_y)/test_y.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
