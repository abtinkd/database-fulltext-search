{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import feature_extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "file_query_features = '/data/khodadaa/stack_results/stack_feat/18_ml_in.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cite \n",
    "# https://stackoverflow.com/questions/17390326/getting-rid-of-stop-words-and-document-tokenization-using-nltk\n",
    "\n",
    "import string\n",
    "stopwords = nltk.corpus.stopwords.words('english') + list(string.punctuation)\n",
    "\n",
    "def tokenize(text: str) -> list:    \n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return [t for t in tokens if t not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Query', 'covered_t_18', 'mean_df_t_18', 'min_df_t_18',\n",
       "       'mean_mean_pop_t_18', 'mean_min_pop_t_18', 'min_mean_pop_t_18',\n",
       "       'min_min_pop_t_18', 'ql_t_18', 'qll_t_18', 'covered_t_bi_18',\n",
       "       'mean_df_t_bi_18', 'min_df_t_bi_18', 'mean_mean_pop_t_bi_18',\n",
       "       'mean_min_pop_t_bi_18', 'min_mean_pop_t_bi_18', 'min_min_pop_t_bi_18',\n",
       "       'ql_t_bi_18', 'qll_t_bi_18', 'scs_t_18', 'maxSCQ_t_18', 'covered_t_c18',\n",
       "       'mean_df_t_c18', 'min_df_t_c18', 'mean_mean_pop_t_c18',\n",
       "       'mean_min_pop_t_c18', 'min_mean_pop_t_c18', 'min_min_pop_t_c18',\n",
       "       'ql_t_c18', 'qll_t_c18', 'covered_t_bi_c18', 'mean_df_t_bi_c18',\n",
       "       'min_df_t_bi_c18', 'mean_mean_pop_t_bi_c18', 'mean_min_pop_t_bi_c18',\n",
       "       'min_mean_pop_t_bi_c18', 'min_min_pop_t_bi_c18', 'ql_t_bi_c18',\n",
       "       'qll_t_bi_c18', 'scs_t_c18', 'maxSCQ_t_c18', 'TestViewCount', '18',\n",
       "       '100', 'ql_t', 'ql_t.1', 'Y', 'Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qf = pd.read_csv(file_query_features)\n",
    "df_qf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_qf['Label']\n",
    "X = df_qf[df_qf.columns.difference(['Label', 'Y'])]\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, stratify=y, test_size= 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 22s, sys: 1.11 s, total: 3min 23s\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%time df_qf['tokens'] = df_qf.Query.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 466 µs, sys: 981 µs, total: 1.45 ms\n",
      "Wall time: 3.47 ms\n",
      "CPU times: user 2min 25s, sys: 3.4 s, total: 2min 28s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "# cite:\n",
    "# http://brandonrose.org/clustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%time tfidf_vectorizer = TfidfVectorizer(max_df=0.9, \\\n",
    "                                         max_features= 5000, \\\n",
    "                                         stop_words=nltk.corpus.stopwords.words('english') + list(string.punctuation), \\\n",
    "                                         lowercase=True, \\\n",
    "                                         use_idf=True, \\\n",
    "                                         tokenizer=nltk.word_tokenize, ngram_range = (1,2))\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(df_qf.loc[train_X.index, 'Query'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52min 21s, sys: 1min 47s, total: 54min 8s\n",
      "Wall time: 10min 43s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "num_clusters = 20\n",
    "km = KMeans(n_clusters = num_clusters, random_state=5, max_iter=100)\n",
    "joblib.dump(km, '/data/khodadaa/stack_results/cluster_train.pkl')\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_X = train_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_X['cluster'] = clusters\n",
    "c1 = n_X[train_y==1]\n",
    "c0 = n_X[train_y==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.580052\n",
       "14    0.052362\n",
       "1     0.044727\n",
       "16    0.036780\n",
       "12    0.033565\n",
       "4     0.029369\n",
       "2     0.027771\n",
       "19    0.025402\n",
       "17    0.019771\n",
       "9     0.018167\n",
       "13    0.018038\n",
       "18    0.016671\n",
       "6     0.015040\n",
       "10    0.014241\n",
       "8     0.014180\n",
       "3     0.013023\n",
       "5     0.012833\n",
       "15    0.012576\n",
       "7     0.008474\n",
       "11    0.006958\n",
       "Name: cluster, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c0['cluster'].value_counts()/c0.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.598871\n",
       "14    0.053302\n",
       "1     0.042088\n",
       "16    0.034665\n",
       "12    0.030726\n",
       "4     0.028037\n",
       "19    0.023864\n",
       "13    0.021571\n",
       "17    0.019241\n",
       "2     0.018431\n",
       "9     0.015961\n",
       "10    0.015295\n",
       "18    0.014776\n",
       "15    0.014713\n",
       "3     0.014653\n",
       "8     0.014026\n",
       "6     0.011922\n",
       "5     0.011177\n",
       "7     0.008643\n",
       "11    0.008036\n",
       "Name: cluster, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1['cluster'].value_counts()/c1.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
