{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os.path import expanduser, join\n",
    "from time import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from pyrallel.ensemble import EnsembleGrower\n",
    "from pyrallel.ensemble import sub_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.parallel import Client\n",
    "lb_view = Client().load_balanced_view()\n",
    "len(lb_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a NumPy array version of Fold1 of the [MSLR-WEB10K](http://research.microsoft.com/en-us/projects/mslr/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = np.load(expanduser('/data/khodadaa/mslr/MSLR-WEB10K/mslr-web10kFold1.npz'))\n",
    "X_train, y_train, qid_train = data['X_train'], data['y_train'], data['qid_train']\n",
    "X_vali, y_vali, qid_vali = data['X_vali'], data['y_vali'], data['qid_vali']\n",
    "X_test, y_test, qid_test = data['X_test'], data['y_test'], data['qid_test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total size in bytes, total number of search results and number of queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train.nbytes + X_vali.nbytes + X_test.nbytes) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train) + len(X_vali) + len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(qid_train)) + len(np.unique(qid_vali)) + len(np.unique(qid_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the training and validation sets as a big development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = np.vstack([X_train, X_vali])\n",
    "y_dev = np.concatenate([y_train, y_vali])\n",
    "qid_dev = np.concatenate([qid_train, qid_vali])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a subset of 500 queries to speed up the learning when prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(X, y, qid, size, seed=None):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_qid = np.unique(qid)\n",
    "    qid_mask = rng.permutation(len(unique_qid))[:size]\n",
    "    subset_mask = np.in1d(qid_train, unique_qid[qid_mask])\n",
    "    return X[subset_mask], y[subset_mask], qid[subset_mask]\n",
    "\n",
    "\n",
    "X_train_small, y_train_small, qid_train_small = subsample(\n",
    "    X_train, y_train, qid_train, 500, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(qid_train_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_medium, y_train_medium, qid_train_medium = subsample(\n",
    "    X_train, y_train, qid_train, 1000, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_irrelevant(X, y, qid, seed=None):\n",
    "    \"\"\"Subsample the zero-scored entries\"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    unique_qid = np.unique(qid)\n",
    "    final_mask = np.ones(shape=y.shape, dtype=np.bool)\n",
    "    for this_qid in unique_qid:\n",
    "        this_mask = qid == this_qid\n",
    "        this_y = y[this_mask]\n",
    "        relevant = this_y >= 2\n",
    "        ratio = float(np.mean(relevant))\n",
    "        if ratio > 0.5:\n",
    "            # already balanced\n",
    "            continue\n",
    "            \n",
    "        final_mask[this_mask] = np.logical_or(\n",
    "            relevant, np.random.random(len(this_y)) > 0.7) \n",
    "    return X[final_mask], y[final_mask], qid[final_mask]\n",
    "\n",
    "X_balanced_small, y_balanced_small, qid_balanced_small = balance_irrelevant(\n",
    "    X_train_small, y_train_small, qid_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(y_train_small))\n",
    "print(len(y_balanced_small))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying ranking success with NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(relevances, rank=10):\n",
    "    \"\"\"Discounted cumulative gain at rank (DCG)\"\"\"\n",
    "    relevances = np.asarray(relevances)[:rank]\n",
    "    n_relevances = len(relevances)\n",
    "    if n_relevances == 0:\n",
    "        return 0.\n",
    "\n",
    "    discounts = np.log2(np.arange(n_relevances) + 2)\n",
    "    return np.sum(relevances / discounts)\n",
    " \n",
    " \n",
    "def ndcg(relevances, rank=10):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDGC)\"\"\"\n",
    "    best_dcg = dcg(sorted(relevances, reverse=True), rank)\n",
    "    if best_dcg == 0:\n",
    "        return 0.\n",
    "\n",
    "    return dcg(relevances, rank) / best_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg([2, 4, 0, 1, 1, 0, 0], rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg([0, 0, 0, 1, 1, 2, 4], rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg([0, 0, 0, 1, 1, 2, 4], rank=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg([4, 2, 1, 1, 0, 0, 0], rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ndcg(y_true, y_pred, query_ids, rank=10):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    query_ids = np.asarray(query_ids)\n",
    "    # assume query_ids are sorted\n",
    "    ndcg_scores = []\n",
    "    previous_qid = query_ids[0]\n",
    "    previous_loc = 0\n",
    "    for loc, qid in enumerate(query_ids):\n",
    "        if previous_qid != qid:\n",
    "            chunk = slice(previous_loc, loc)\n",
    "            ranked_relevances = y_true[chunk][np.argsort(y_pred[chunk])[::-1]]\n",
    "            ndcg_scores.append(ndcg(ranked_relevances, rank=rank))\n",
    "            previous_loc = loc\n",
    "        previous_qid = qid\n",
    "\n",
    "    chunk = slice(previous_loc, loc + 1)\n",
    "    ranked_relevances = y_true[chunk][np.argsort(y_pred[chunk])[::-1]]\n",
    "    ndcg_scores.append(ndcg(ranked_relevances, rank=rank))\n",
    "    return np.mean(ndcg_scores)\n",
    "\n",
    "\n",
    "mean_ndcg([4, 3, 1, 4, 3], [4, 0, 1, 4, 2], [0, 0, 0, 2, 2], rank=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(model, X, y, qid):\n",
    "    tic = time()\n",
    "    y_predicted = model.predict(X)\n",
    "    prediction_time = time() - tic\n",
    "    print(\"Prediction time: {:.3f}s\".format(prediction_time))\n",
    "    print(\"NDCG@5 score: {:.3f}\".format(\n",
    "    mean_ndcg(y, y_predicted, qid, rank=5)))\n",
    "    print(\"NDCG@10 score: {:.3f}\".format(\n",
    "    mean_ndcg(y, y_predicted, qid, rank=10)))\n",
    "    print(\"NDCG score: {:.3f}\".format(\n",
    "    mean_ndcg(y, y_predicted, qid, rank=None)))\n",
    "    print(\"R2 score: {:.3f}\".format(r2_score(y, y_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ndcg_by_trees(model, X, y, qid, rank=10):\n",
    "    max_n_trees = len(model.estimators_)\n",
    "    scores = []\n",
    "    \n",
    "    if hasattr(model, 'staged_predict'):\n",
    "        # stage-wise score computation for boosted ensembles\n",
    "        n_trees = np.arange(max_n_trees) + 1\n",
    "        for y_predicted in model.staged_predict(X):\n",
    "            scores.append(mean_ndcg(y, y_predicted, qid, rank=10))\n",
    "    else:\n",
    "        # assume forest-type of tree ensemble: use a log scale to speedup\n",
    "        # the computation\n",
    "        # XXX: partial predictions could be reused\n",
    "        n_trees = np.logspace(0, np.log10(max_n_trees), 10).astype(int)\n",
    "        for j, n in enumerate(n_trees):\n",
    "            y_predicted = sub_ensemble(model, n).predict(X)\n",
    "            scores.append(mean_ndcg(y, y_predicted, qid, rank=rank))\n",
    "            \n",
    "    plt.plot(n_trees, scores)\n",
    "    plt.xlabel(\"Number of trees\")\n",
    "    plt.ylabel(\"Average NDCG@%d\" % rank)\n",
    "    _ = plt.title(\"Impact of the number of trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "etr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(etr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ndcg_by_trees(etr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "rfr.fit(X_balanced_small, y_balanced_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(rfr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ndcg_by_trees(rfr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr2 = RandomForestRegressor(n_estimators=200, min_samples_split=5, random_state=1, n_jobs=-1)\n",
    "rfr2.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rfr.set_params(n_jobs=-1)\n",
    "print_evaluation(rfr2, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ndcg_by_trees(rfr2, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=200, random_state=1, verbose=1)\n",
    "gbr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(gbr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ndcg_by_trees(gbr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr2 = GradientBoostingRegressor(n_estimators=300, max_depth=3,\n",
    "                                 learning_rate=0.1, loss='ls',\n",
    "                                 random_state=1, verbose=1)\n",
    "gbr2.fit(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(gbr2, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ndcg_by_trees(gbr2, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(gbr2, X_dev, y_dev, qid_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import LambdaMART\n",
    "\n",
    "lmart= LambdaMART(n_estimators=300, max_depth=3,\n",
    "                  learning_rate=0.1, random_state=1, verbose=1)\n",
    "lmart.fit(X_train_small, y_train_small, group=qid_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(lmart, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ndcg_by_trees(lmart, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(lmart, X_train_small, y_train_small, qid_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lmart= LambdaMART(n_estimators=300, max_depth=3,\n",
    "                  learning_rate=0.1, random_state=1, verbose=1)\n",
    "lmart.fit(X_dev, y_dev, group=qid_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(lmart, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(lmart, X_dev, y_dev, qid_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ndcg_by_trees(lmart, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Growing Randomized Trees to predict relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "base_estimator = RandomForestRegressor(n_estimators=1, min_samples_split=10)\n",
    "grower = EnsembleGrower(lb_view, base_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grower.launch(X_dev, y_dev, n_estimators=200, folder=\"web10k\",\n",
    "              dump_models=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grower.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rfr = grower.aggregate_model()\n",
    "print(\"Number of trees: {}\".format(len(rfr.estimators_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(rfr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ndcg_by_trees(rfr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the overfitting of the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(rfr, X_train_medium, y_train_medium, qid_train_medium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with a baseline linear regression models (with different optimizers and input scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lr = LinearRegression().fit(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time y_test_lr = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(lr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate overfitting by comparing with training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(lr, X_dev, y_dev, qid_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, a slight overfitting of the training set from a regression standpoint (higher r2 score) does not seem to cause overfitting from a ranking standpoint. This would have to be checked with cross-validation though.\n",
    "\n",
    "Let's evaluate the impact of imput feature normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nlr = LinearRegression(normalize=True).fit(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(nlr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nlr = LinearRegression(normalize=True).fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(nlr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_small_scaled = scaler.fit_transform(X_train_small)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sgdlr = SGDRegressor(alpha=1e-7, learning_rate='constant', eta0=1e-5, n_iter=50).fit(X_train_small_scaled, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(sgdlr, X_test_scaled, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with a classification to NDCG ranking reduction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba_to_relevance(probas):\n",
    "    \"\"\"MCRank-like reduction of classification proba to DCG predictions\"\"\"\n",
    "    rel = np.zeros(probas.shape[0], dtype=np.float32)\n",
    "    for i in range(probas.shape[1]):\n",
    "        rel += i * probas[:, i]\n",
    "    return rel\n",
    "        \n",
    "        \n",
    "class ClassificationRanker(RegressorMixin):\n",
    "    \n",
    "    def __init__(self, base_estimator=None):\n",
    "        self.base_estimator = base_estimator\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.estimator_ = clone(self.base_estimator)\n",
    "        self.scaler_ = StandardScaler()\n",
    "        X = self.scaler_.fit_transform(X)\n",
    "        self.estimator_.fit(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_scaled = self.scaler_.transform(X)\n",
    "        probas = self.estimator_.predict_proba(X_scaled)\n",
    "        return proba_to_relevance(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "logr = ClassificationRanker(LogisticRegression(C=1000))\n",
    "logr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(logr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgdlogr = SGDClassifier(loss='modified_huber', alpha=1e-8, n_iter=200, learning_rate='constant', eta0=1e-6, n_jobs=-1)\n",
    "sgdlogrr = ClassificationRanker(sgdlogr)\n",
    "sgdlogrr.fit(X_train_small_scaled, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(sgdlogrr, X_test_scaled, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200, max_features=20, min_samples_split=5,\n",
    "                            random_state=1, n_jobs=-1)\n",
    "rfr = ClassificationRanker(rfc)\n",
    "rfr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(rfr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, random_state=1)\n",
    "gbr = ClassificationRanker(gbc)\n",
    "gbr.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(gbr, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ClassificationRanker(ExtraTreesClassifier(n_estimators=200, random_state=1, n_jobs=-1))\n",
    "etc.fit(X_train_small, y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_evaluation(etc, X_test, y_test, qid_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introspecting the distribution of relevance scores predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = np.random.permutation(y_test.shape[0])[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Extra Trees predictions')\n",
    "plt.scatter(y_test[subset], y_test_etr[subset], alpha=0.1, s=100)\n",
    "plt.xlabel('True relevance')\n",
    "plt.ylabel('Predicted relevance')\n",
    "plt.ylim(-2, 5)\n",
    "plt.xlim(-2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Linear Regression predictions')\n",
    "plt.scatter(y_test[subset], y_test_lr[subset], alpha=0.1, s=100)\n",
    "plt.xlabel('True relevance')\n",
    "plt.ylabel('Predicted relevance')\n",
    "plt.ylim(-2, 5)\n",
    "plt.xlim(-2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test, bins=5, alpha=.3, color='b', label='True relevance')\n",
    "plt.hist(y_test_etr, bins=5, alpha=.3, color='g', label='ET predicted relevance')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each query, count the number of results with rank 0, 1, 2, 3 or 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_qids_test = np.unique(qid_test)\n",
    "for qid in unique_qids_test[:5]:\n",
    "    qids = y_test[qid_test == qid].astype(np.int)\n",
    "    print(np.bincount(qids, minlength=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
